{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8-yl-s-WKMG"
   },
   "source": [
    "# Object Detection API Demo\n",
    "\n",
    "<table align=\"left\"><td>\n",
    "  <a target=\"_blank\"  href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab\n",
    "  </a>\n",
    "</td><td>\n",
    "  <a target=\"_blank\"  href=\"https://colab.sandbox.google.com/github/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\">\n",
    "    <img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "</td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3cIrseUv6WKz"
   },
   "source": [
    "Welcome to the [Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection). This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VrJaG0cYN9yh"
   },
   "source": [
    "> **Important**: This tutorial is to help you through the first step towards using [Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) to build models. If you just just need an off the shelf model that does the job, see the [TFHub object detection example](https://colab.sandbox.google.com/github/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFSqkTCdWKMI"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awjrpqy-6MaQ"
   },
   "source": [
    "Important: If you're running on a local machine, be sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md). This notebook includes only what's necessary to run in Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p3UGXxUii5Ym"
   },
   "source": [
    "### Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-vsOL3QR6kqs"
   },
   "source": [
    "Get `tensorflow/models` or `cd` to parent directory of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresh_hold value is taken as 0.3 for person and car detection\n",
    "thresh_hold_val = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykA0c-om51s1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'git' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O219m6yWAj9l"
   },
   "source": [
    "Compile protobufs and install the object_detection package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LBdjK2G5ywuc"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hV4P5gyTWKMI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from lxml import etree\n",
    "import xml.etree.cElementTree as ET\n",
    "from tkinter import Tk, Label,Entry\n",
    "from tkinter import filedialog,ttk,StringVar\n",
    "from tkinter import Checkbutton\n",
    "from tkinter import Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter.messagebox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5FNuiRPWKMN"
   },
   "source": [
    "Import the object detection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4-IMl4b6BdGO"
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RYPCiag2iz_q"
   },
   "source": [
    "Patches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mF-YlMl8c_bM"
   },
   "outputs": [],
   "source": [
    "# patch tf1 into `utils.ops`\n",
    "utils_ops.tf = tf.compat.v1\n",
    "\n",
    "# Patch the location of gfile\n",
    "tf.gfile = tf.io.gfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfn_tRFOWKMO"
   },
   "source": [
    "# Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_sEBLpVWKMQ"
   },
   "source": [
    "## Variables\n",
    "\n",
    "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing the path.\n",
    "\n",
    "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ai8pLZZWKMS"
   },
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zm8xp-0eoItE"
   },
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "  base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
    "  model_file = model_name + '.tar.gz'\n",
    "  model_dir = tf.keras.utils.get_file(\n",
    "    fname=model_name, \n",
    "    origin=base_url + model_file,\n",
    "    untar=True)\n",
    "\n",
    "  model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
    "\n",
    "  model = tf.saved_model.load(str(model_dir))\n",
    "  model = model.signatures['serving_default']\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1MVVTcLWKMW"
   },
   "source": [
    "## Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hDbpHkiWWKMX"
   },
   "outputs": [],
   "source": [
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = r'F:\\models-master\\research\\object_detection\\data\\mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVU3U_J6IJVb"
   },
   "source": [
    "For the sake of simplicity we will test on 2 images:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0_1AGhrWKMc"
   },
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f7aOtOlebK7h"
   },
   "source": [
    "Load an object detection model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1XNT0wxybKR6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "model_name = 'ssd_mobilenet_v1_coco_2018_01_28'\n",
    "detection_model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yN1AYfAEJIGp"
   },
   "source": [
    "Check the model's input signature, it expects a batch of 3-color images of type uint8: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1wq0LVyMRR_"
   },
   "source": [
    "Run it on each test image and show the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## realtime object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before running this code pls go to utils folder in object detection directory and replace existing visualization_utils file with new visualization_utils.py file given along with this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "global videofile  # I used global variables to take videofile as video directory input and location as image save directory input\n",
    "global location  #global variables are used since same variables are used inside the play video function\n",
    "\n",
    "    \n",
    "def opendir():\n",
    "        root.directory = filedialog.askdirectory() #filedialog.askdirectory() takes the location name of input directory\n",
    "        global videofile\n",
    "        videofile=root.directory\n",
    "\n",
    "def filepath():        \n",
    "    filename = filedialog.askdirectory()\n",
    "    global location\n",
    "    location=filename\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # the following function playvideo takes the inputs from GUI's and saves image in incremental order with names  after every nth frame. And also creates 2 seperate folder for person and car to store only the cropped images of car and person class. And then checks for the check boxes of which format label to be saved like xml,kitti,yolo or all and creates respective label folders and saves label file with same name as frame image.\n",
    "#note : if there are no person or car found in the image then label file for that image will not be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def playvideo():       # This function will be input to the play video button\n",
    "    \n",
    "    out_format = [var1.get(),var2.get(),var3.get(),var4.get()]  #returns the name of checked and unchecked check boxes\n",
    "    output_format = []          #This array holds only checked boxes\n",
    "    for i in out_format:\n",
    "        if i !='0' :\n",
    "            output_format.append(i)\n",
    "    \n",
    "    if output_format == []:     #if no checkbox is checked then it gives warning messege\n",
    "        tkinter.messagebox.showinfo('warning','pls check atleast one value in checkbox')\n",
    "\n",
    "    frame_number = int(t1.get())    #it takes integer frame number from GUI input\n",
    "    \n",
    "    if output_format != []:    #if any one or more checkboxes are checked then following code will be executed\n",
    "    \n",
    "        videoslist = os.listdir(videofile)   #lists the given input video directory\n",
    "        videos = []                 #creates empty array to hold only .mp4 files\n",
    "        for i in videoslist:\n",
    "            if i.endswith('.mp4'):\n",
    "                videos.append(videofile+'/'+i)\n",
    "        countid2= 1              #the name of image begins with number 1\n",
    "        zero_array = [0]*2       #creates empty zero array to hold the count of car and persons\n",
    "        for i in videos:\n",
    "\n",
    "            cap = cv2.VideoCapture(i)   #capture video from video file\n",
    "            countid= countid2           #initialize countid = prvious count id (countid2)\n",
    "            count = 0                  #count = 0 for the start of video frame\n",
    "            new_array = zero_array     #initialize new array to zero_array\n",
    "            items = ['person','car']  #person and car are put into the array since we have known object classes to detect\n",
    "            sourceDirectory = location   #this location saves image to the location directory in GUI input\n",
    "\n",
    "\n",
    "            while cap.isOpened():\n",
    "                ret, image_np = cap.read()\n",
    "                ret1,frame = cap.read()\n",
    "                  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "                if ret:\n",
    "                    image = np.asarray(image_np)\n",
    "                  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "                    input_tensor = tf.convert_to_tensor(image)\n",
    "                  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "                    input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "                  # Run inference\n",
    "                    output_dict = detection_model(input_tensor)\n",
    "\n",
    "                  # All outputs are batches tensors.\n",
    "                  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "                  # We're only interested in the first num_detections.\n",
    "                    num_detections = int(output_dict.pop('num_detections'))\n",
    "                    output_dict = {key:value[0, :num_detections].numpy() \n",
    "                                 for key,value in output_dict.items()}\n",
    "                    output_dict['num_detections'] = num_detections\n",
    "\n",
    "                  # detection_classes should be ints.\n",
    "                    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "                      # Visualization of the results of a detection.\n",
    "                    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                      image_np,\n",
    "                      output_dict['detection_boxes'],\n",
    "                      output_dict['detection_classes'],\n",
    "                      output_dict['detection_scores'],\n",
    "                      category_index,\n",
    "                      instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "                      use_normalized_coordinates=True,\n",
    "                      min_score_thresh=thresh_hold_val,\n",
    "                      line_thickness=8)\n",
    "\n",
    "\n",
    "                   #get classes,classname and bounding boxes from visualization utils.py i have made changes to visualization utils.py\n",
    "                    image,boxes,classname,classes = vis_util.visualize_boxes_and_labels_on_image_array(image_np,\n",
    "                                                                                   output_dict['detection_boxes'],\n",
    "                                                                                   output_dict['detection_classes'],\n",
    "                                                                                   output_dict['detection_scores'],\n",
    "                                                                                   category_index,\n",
    "                                                                                   instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "                                                                                   use_normalized_coordinates=True,\n",
    "                                                                                   min_score_thresh=thresh_hold_val,\n",
    "                                                                                   line_thickness=8)\n",
    "\n",
    "                    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  #convert opencv bgr to rgb format\n",
    "\n",
    "\n",
    "\n",
    "                    im = Image.fromarray(img)      #read the frame into pil format\n",
    "                    width,height = im.size         #get width and height\n",
    "\n",
    "                    #xml label format\n",
    "                    annotation = ET.Element('annotation')           \n",
    "                    ET.SubElement(annotation, 'folder').text = 'xml labels'\n",
    "                    ET.SubElement(annotation, 'filename').text = str(countid)+'.jpg'\n",
    "                    ET.SubElement(annotation, 'segmented').text = '0'\n",
    "                    size = ET.SubElement(annotation, 'size')\n",
    "                    ET.SubElement(size, 'width').text = str(width)\n",
    "                    ET.SubElement(size, 'height').text = str(height)\n",
    "                    ET.SubElement(size, 'depth').text = '3'\n",
    "\n",
    "\n",
    "\n",
    "                    lines = []          #lines to store items in image in kitti format\n",
    "                    yolotext = []       #yolotext stores image in yolo format\n",
    "                    for i in boxes:\n",
    "                      left =  int(i[1]*width)       #get left,right,top,bottom values from boxes\n",
    "                      bottom = int(i[2]*height)\n",
    "                      right =int(i[3]*width)\n",
    "                      top = int(i[0]*height)\n",
    "                      val = (left, top, right, bottom)\n",
    "                      im1 = im.crop((left, top, right, bottom))   #crop car or person from frame \n",
    "\n",
    "                      index = boxes.index(i)          \n",
    "                      object_name = classname[index]          #get the object name like car or person from array\n",
    "                      classid = classes[index]                #get the class id\n",
    "                      if classid == 3:\n",
    "                          classid = 0\n",
    "                      path = location+'/'+object_name    #creates folder with object name\n",
    "\n",
    "                      objindex= items.index(object_name)\n",
    "                      new_array[objindex] +=1\n",
    "                      a = new_array[objindex]  \n",
    "\n",
    "                      dw = 1./width      #there is standard formula for yolo format\n",
    "                      dh = 1./height\n",
    "\n",
    "                      x = (left+right)/2.0\n",
    "                      y = (top+bottom)/2.0\n",
    "                      w = right - left\n",
    "                      h = bottom - top\n",
    "\n",
    "                      x = x*dw      #x,y are centers w and h are width and height for yolo format\n",
    "                      y = y*dh\n",
    "                      w = w*dw\n",
    "                      h = h*dh\n",
    "\n",
    "                      yolotext.append(str(classid)+' '+str(round(x,3))+' '+str(round(y,3))+' '+str(round(w,3))+' '+str(round(h,3)))\n",
    "                      lines.append(object_name + ' ' + '0.00 0 0.00 ' +str(left) + ' ' + str(top) + ' ' + str(right) + ' ' + str(bottom) + ' 0.00 0.00 0.00 0.00 0.00 0.00 0.00')\n",
    "\n",
    "                      if not os.path.exists(path):\n",
    "                         os.makedirs(path)\n",
    "                      im1.save(path+'/'+object_name+str(a)+'.jpg')  \n",
    "\n",
    "                      ob = ET.SubElement(annotation, 'object')\n",
    "                      ET.SubElement(ob, 'name').text = object_name\n",
    "                      ET.SubElement(ob, 'pose').text = 'Unspecified'\n",
    "                      ET.SubElement(ob, 'truncated').text = '0'\n",
    "                      ET.SubElement(ob, 'difficult').text = '0'\n",
    "                      bbox = ET.SubElement(ob, 'bndbox')\n",
    "                      ET.SubElement(bbox, 'xmin').text = str(left)\n",
    "                      ET.SubElement(bbox, 'ymin').text = str(top)\n",
    "                      ET.SubElement(bbox, 'xmax').text = str(right)\n",
    "                      ET.SubElement(bbox, 'ymax').text = str(bottom)\n",
    "\n",
    "\n",
    "                    xml_str = ET.tostring(annotation)\n",
    "                    root = etree.fromstring(xml_str)\n",
    "                    xml_str = etree.tostring(root, pretty_print=True)\n",
    "\n",
    "\n",
    "#check if 'xml' checkbox is checked or not and based on the given input it creates respective folders and label files in folder\n",
    "                    if 'xml' in output_format or 'all' in output_format:\n",
    "\n",
    "                        Directory1 = location+'/'+'xml labels'\n",
    "                        if not os.path.exists(Directory1):\n",
    "                            os.makedirs(Directory1)\n",
    "\n",
    "                        if boxes != []:\n",
    "                            xml_labels = str(countid) + '.xml'\n",
    "                            save_path = os.path.join(Directory1, xml_labels)\n",
    "                            with open(save_path, 'wb') as temp_xml:\n",
    "                                temp_xml.write(xml_str)\n",
    "\n",
    "\n",
    "#check if 'yolo' checkbox is checked or not and based on the given input it creates respective folders and label files in folder\n",
    "\n",
    "                    if 'yolo' in output_format or 'all' in output_format:\n",
    "\n",
    "                        Directory2 = location+'/'+'yolo labels'\n",
    "                        if not os.path.exists(Directory2):\n",
    "                            os.makedirs(Directory2)\n",
    "\n",
    "                        if yolotext != []:\n",
    "                            yolo_label = str(countid)+ '.txt'\n",
    "                            with open(os.path.join(Directory2, yolo_label), \"w\") as text12_file:\n",
    "                                for item in yolotext:\n",
    "                                    text12_file.write(\"%s\\n\" % item)              \n",
    "\n",
    "#check if 'kitti' checkbox is checked or not and based on the given input it creates respective folders and label files in folder\n",
    "                                    \n",
    "                    if 'kitti' in output_format or 'all' in output_format:\n",
    "\n",
    "                        Directory3 = location+'/'+'kitty labels'\n",
    "                        if not os.path.exists(Directory3):\n",
    "                            os.makedirs(Directory3)\n",
    "\n",
    "                        if lines != []:\n",
    "                            kitti_labels = str(countid) + '.txt'\n",
    "                            with open(os.path.join(Directory3, kitti_labels), \"w\") as text_file:\n",
    "                                #if lines is not []:\n",
    "                                for item in lines:\n",
    "                                    text_file.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "                    im.save(sourceDirectory+'/'+str(countid)+'.jpg')\n",
    "                    count += frame_number         #30 # i.e. at 30 fps, this advances one second\n",
    "                    countid+=1                    #increases the countid for next image to be saved\n",
    "                    cap.set(1, count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    cv2.imshow('object detection',image_np)\n",
    "                    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                else:\n",
    "                    cap.release()\n",
    "                    break\n",
    "            #cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "            zero_array = new_array     #for the next video initialize zero_array = new_array\n",
    "            countid2=countid+1       #initialize countid2 for next video to last image count of previous video+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following code creates GUI app to take the input and then executes the above function based on the given input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Tk()       #create Gui window using tkinter\n",
    "root.wm_title(\"Objecct detection\")   #name of GUI window\n",
    "root.geometry(\"400x300\")            #Geometry size for GUI\n",
    "\n",
    "l2=Label(root,text=\"enter the frame number\")    #label to enter frame number in gui\n",
    "l2.grid(row=1,column=0)                         #location of the label\n",
    "t1=Entry(root)                                  #Entry command is used to take the input for frame number\n",
    "t1.grid(row=1,column=1)                         #location of entry window\n",
    "\n",
    "\n",
    "Label(root, text ='Select format u need pls uncheck all the boxes 1st if checked then check the boxes u need').place(x = 20, y = 180) \n",
    "  \n",
    "# check buttons \n",
    "var1 = StringVar()  #var1 is used return the checked and unchecked value\n",
    "xml = Checkbutton(root, text ='xml', \n",
    "                   variable = var1,onvalue = 'xml',offvalue = None).place(x = 40, y = 200) \n",
    "\n",
    "var2 = StringVar()\n",
    "yolo = Checkbutton(root, text ='yolo', \n",
    "                  variable = var2,onvalue = 'yolo',offvalue = None).place(x = 40, y = 220) \n",
    "\n",
    "var3 = StringVar()\n",
    "kitti = Checkbutton(root, text ='kitti', \n",
    "                     variable =var3,onvalue = 'kitti',offvalue = None).place(x = 40, y = 240) \n",
    "\n",
    "var4 = StringVar()\n",
    "all = Checkbutton(root, text ='all', \n",
    "                variable = var4,onvalue = 'all',offvalue = None).place(x = 40, y = 260) \n",
    "\n",
    "#The following function creates Buttons when clicked performs the given in command variable\n",
    "B = Button(text =\"Open video path\",command=opendir,height = 1, width = 15).place(x=40,y=50)\n",
    "B1 = Button(text =\"Open image save path\",command=filepath,height = 1, width = 18).place(x=180,y=50)\n",
    "B2  =  Button(text =\"Play video\",command=playvideo,height = 2, width = 20).place(x=100,y=100)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/brain/python/client:colab_notebook",
    "kind": "private"
   },
   "name": "object_detection_tutorial.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1LNYL6Zsn9Xlil2CVNOTsgDZQSBKeOjCh",
     "timestamp": 1566498233247
    },
    {
     "file_id": "/piper/depot/google3/third_party/tensorflow_models/object_detection/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
     "timestamp": 1566488313397
    },
    {
     "file_id": "/piper/depot/google3/third_party/py/tensorflow_docs/g3doc/en/r2/tutorials/generative/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
     "timestamp": 1566145894046
    },
    {
     "file_id": "1nBPoWynOV0auSIy40eQcBIk9C6YRSkI8",
     "timestamp": 1566145841085
    },
    {
     "file_id": "/piper/depot/google3/third_party/tensorflow_models/object_detection/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
     "timestamp": 1556295408037
    },
    {
     "file_id": "1layerger-51XwWOwYMY_5zHaCavCeQkO",
     "timestamp": 1556214267924
    },
    {
     "file_id": "/piper/depot/google3/third_party/tensorflow_models/object_detection/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
     "timestamp": 1556207836484
    },
    {
     "file_id": "1w6mqQiNV3liPIX70NOgitOlDF1_4sRMw",
     "timestamp": 1556154824101
    },
    {
     "file_id": "https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb",
     "timestamp": 1556150293326
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
